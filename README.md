# ğŸ“„ RAG Chatbot â€” PDF Question Answering System

Upload any PDF â†’ Ask questions in natural language â†’ Get accurate answers with source citations.

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-1.2+-green?logo=chainlink&logoColor=white)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--3.5-orange?logo=openai&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-1.38+-red?logo=streamlit&logoColor=white)

---

## ğŸ“¸ Demo

![RAG Chatbot Demo](rag-chatbot/screenshots/rag.png)

---

## ğŸ¯ What It Does

A Retrieval-Augmented Generation (RAG) chatbot that reads your PDF documents and answers questions about them. Every answer comes with source citations showing exactly which page and chunk the information came from.

**Example:**
```
ğŸ“„ Uploaded: company_handbook.pdf

ğŸ§‘ Q: "What is the leave policy for employees?"
ğŸ¤– A: "All full-time employees are entitled to 24 days of Paid Leave,
       12 days of Sick Leave, 10 Public Holidays, and 5 days of Casual Leave
       per calendar year..."
ğŸ“š Source: Page 2, Chunk 3
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Upload  â”‚â”€â”€â”€â”€â–¶â”‚  Text Extraction â”‚â”€â”€â”€â”€â–¶â”‚   Chunking   â”‚
â”‚  (PyPDF)     â”‚     â”‚  (page by page)  â”‚     â”‚  (500 chars) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Answer     â”‚â—€â”€â”€â”€â”€â”‚   LLM (GPT-3.5) â”‚â—€â”€â”€â”€â”€â”‚  ChromaDB    â”‚
â”‚  + Sources   â”‚     â”‚ Context + Query  â”‚     â”‚  (Vectors)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â–²
                                                      â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                              â”‚  User Query  â”‚
                                              â”‚  (Embedding) â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RAG Pipeline Steps:**

1. **Load** â€” PDF pages extracted via PyPDFLoader
2. **Chunk** â€” Text split into 500-char overlapping chunks
3. **Embed** â€” Each chunk converted to vector embedding (OpenAI text-embedding-ada-002)
4. **Store** â€” Vectors indexed in ChromaDB for similarity search
5. **Retrieve** â€” User query embedded â†’ top-4 similar chunks retrieved
6. **Generate** â€” Retrieved chunks + query + chat history â†’ GPT-3.5-turbo â†’ answer

---

## ğŸ› ï¸ Tech Stack

| Component       | Technology                | Purpose                              |
|-----------------|---------------------------|--------------------------------------|
| Framework       | LangChain 1.2+            | RAG pipeline orchestration           |
| Vector Store    | ChromaDB                  | Embedding storage & similarity search|
| LLM             | OpenAI GPT-3.5-turbo      | Answer generation                    |
| Embeddings      | OpenAI text-embedding-ada-002 | Text â†’ vector conversion         |
| PDF Parsing     | PyPDF                     | PDF text extraction                  |
| UI              | Streamlit                 | Chat interface                       |
| Chat Memory     | LangChain Messages        | Multi-turn conversation support      |

---

## ğŸš€ Quick Start

### 1. Clone the repo
```bash
git clone 
cd rag-chatbot
```

### 2. Create virtual environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate.bat

# Mac/Linux
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up API key
```bash
cp .env.example .env
```
Open `.env` and add your OpenAI API key:
```
OPENAI_API_KEY=sk-your-api-key-here
```
Get your key at [platform.openai.com/api-keys](https://platform.openai.com/api-keys)

### 5. Run the app
```bash
streamlit run app.py
```
App opens at `http://localhost:8501`

### 6. Test (optional â€” CLI mode)
```bash
# Place any PDF in data/ folder first
python rag_engine.py
```

---

## ğŸ“ Project Structure

```
rag-chatbot/
â”œâ”€â”€ rag_engine.py       # Core RAG pipeline (load â†’ chunk â†’ embed â†’ retrieve â†’ generate)
â”œâ”€â”€ app.py              # Streamlit chat interface
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env.example        # API key template
â”œâ”€â”€ .gitignore          # Git ignore rules
â”œâ”€â”€ data/               # Sample PDFs for testing
â”‚   â””â”€â”€ sample.pdf      # NovaTech Company Handbook (test document)
â”œâ”€â”€ screenshots/        # App screenshots
â”‚   â””â”€â”€ rag.png         # Demo screenshot
â””â”€â”€ README.md
```

---

## âœ¨ Key Features

- **Multi-PDF Support** â€” Upload and query across multiple documents at once
- **Source Citations** â€” Every answer shows which page and chunk was used
- **Chat Memory** â€” Remembers conversation for follow-up questions
- **Chunk Overlap** â€” 50-char overlap prevents context loss at boundaries
- **Similarity Search** â€” Top-4 most relevant chunks retrieved per query
- **Reset Option** â€” Clear everything and start fresh with new documents

---

## âš™ï¸ Configuration

Tunable parameters in `rag_engine.py`:

| Parameter        | Default | What it controls                          |
|------------------|---------|-------------------------------------------|
| `chunk_size`     | 500     | Characters per chunk (smaller = precise)  |
| `chunk_overlap`  | 50      | Overlap between chunks                    |
| `k` (retriever)  | 4       | Number of chunks retrieved per query      |
| `temperature`    | 0       | LLM randomness (0 = deterministic)        |
| `model`          | gpt-3.5-turbo | OpenAI model used                   |

---

## ğŸ”® Future Improvements

- [ ] Support for DOCX, TXT, and web URLs
- [ ] Hybrid search (keyword + semantic)
- [ ] Re-ranking with cross-encoder
- [ ] Deploy on Hugging Face Spaces
- [ ] Evaluation metrics (retrieval accuracy, answer faithfulness)
- [ ] Upgrade to Agentic RAG with LangGraph

---

## ğŸ“ What I Learned

- How text embeddings capture semantic meaning and enable similarity search
- The importance of chunk size and overlap in retrieval quality
- How chat history enables multi-turn conversations in RAG systems
- Trade-offs between retrieval precision vs recall when tuning `k` parameter
- Why source attribution matters for trustworthy AI applications

---

## ğŸ“„ License

MIT
